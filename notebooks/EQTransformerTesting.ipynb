{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aYMejiGTuwQ"
      },
      "outputs": [],
      "source": [
        "!pip install seisbench"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from queue import PriorityQueue\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "import bottleneck as bn\n",
        "import nest_asyncio\n",
        "import numpy as np\n",
        "import obspy\n",
        "import torch\n",
        "import torch.multiprocessing as torchmp\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from obspy.signal.trigger import trigger_onset\n",
        "from packaging import version\n",
        "\n",
        "import seisbench\n",
        "import seisbench.util as util\n",
        "from seisbench.util import in_notebook, log_lifecycle\n",
        "\n",
        "from scipy import signal\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "jvhGVCgXVh56"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layers"
      ],
      "metadata": {
        "id": "UrrsVKy0UgY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class CustomLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM to be used with custom cells\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cell, *cell_args, bidirectional=True, **cell_kwargs):\n",
        "        super(CustomLSTM, self).__init__()\n",
        "        self.cell_f = cell(*cell_args, **cell_kwargs)\n",
        "        self.bidirectional = bidirectional\n",
        "        if self.bidirectional:\n",
        "            self.cell_b = cell(*cell_args, **cell_kwargs)\n",
        "\n",
        "    def forward(self, input, state=None):\n",
        "        # Forward\n",
        "        state_f = state\n",
        "        outputs_f = []\n",
        "        for i in range(len(input)):\n",
        "            out, state_f = self.cell_f(input[i], state_f)\n",
        "            outputs_f += [out]\n",
        "\n",
        "        outputs_f = torch.stack(outputs_f)\n",
        "\n",
        "        if not self.bidirectional:\n",
        "            return outputs_f, None\n",
        "\n",
        "        # Backward\n",
        "        state_b = state\n",
        "        outputs_b = []\n",
        "        l = input.shape[0] - 1\n",
        "        for i in range(len(input)):\n",
        "            out, state_b = self.cell_b(input[l - i], state_b)\n",
        "            outputs_b += [out]\n",
        "\n",
        "        outputs_b = torch.flip(torch.stack(outputs_b), dims=[0])\n",
        "\n",
        "        output = torch.cat([outputs_f, outputs_b], dim=-1)\n",
        "\n",
        "        # Keep second argument for consistency with PyTorch LSTM\n",
        "        return output, None"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Dvu2bsBoUjb5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def hard_sigmoid(x):\n",
        "    return torch.clip(0.2 * x + 0.5, 0, 1)\n",
        "\n",
        "class ActivationLSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM Cell using variable gating activation, by default hard sigmoid\n",
        "\n",
        "    If gate_activation=torch.sigmoid this is the standard LSTM cell\n",
        "\n",
        "    Uses recurrent dropout strategy from https://arxiv.org/abs/1603.05118 to match Keras implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, input_size, hidden_size, gate_activation=hard_sigmoid, recurrent_dropout=0\n",
        "    ):\n",
        "        super(ActivationLSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gate_activation = gate_activation\n",
        "        self.recurrent_dropout = recurrent_dropout\n",
        "\n",
        "        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size))\n",
        "        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\n",
        "        self.bias_ih = nn.Parameter(torch.randn(4 * hidden_size))\n",
        "        self.bias_hh = nn.Parameter(torch.randn(4 * hidden_size))\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            for param in [self.weight_hh, self.weight_ih]:\n",
        "                for idx in range(4):\n",
        "                    mul = param.shape[0] // 4\n",
        "                    torch.nn.init.xavier_uniform_(param[idx * mul : (idx + 1) * mul])\n",
        "\n",
        "    def forward(self, input, state):\n",
        "        if state is None:\n",
        "            hx = torch.zeros(\n",
        "                input.shape[0], self.hidden_size, device=input.device, dtype=input.dtype\n",
        "            )\n",
        "            cx = torch.zeros(\n",
        "                input.shape[0], self.hidden_size, device=input.device, dtype=input.dtype\n",
        "            )\n",
        "        else:\n",
        "            hx, cx = state\n",
        "        gates = (\n",
        "            torch.mm(input, self.weight_ih.t())\n",
        "            + self.bias_ih\n",
        "            + torch.mm(hx, self.weight_hh.t())\n",
        "            + self.bias_hh\n",
        "        )\n",
        "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
        "\n",
        "        ingate = self.gate_activation(ingate)\n",
        "        forgetgate = self.gate_activation(forgetgate)\n",
        "        cellgate = torch.tanh(cellgate)\n",
        "        outgate = self.gate_activation(outgate)\n",
        "\n",
        "        if self.recurrent_dropout > 0:\n",
        "            cellgate = F.dropout(cellgate, p=self.recurrent_dropout)\n",
        "\n",
        "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
        "        hy = outgate * torch.tanh(cy)\n",
        "\n",
        "        return hy, (hy, cy)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A6z2MfsfUt4u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "import seisbench.util as sbu\n",
        "\n",
        "# For implementation, potentially follow: https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28\n",
        "class EQTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    The EQTransformer from Mousavi et al. (2020)\n",
        "\n",
        "    Implementation adapted from the Github repository https://github.com/smousavi05/EQTransformer\n",
        "    Assumes padding=\"same\" and activation=\"relu\" as in the pretrained EQTransformer models\n",
        "\n",
        "    By instantiating the model with `from_pretrained(\"original\")` a binary compatible version of the original\n",
        "    EQTransformer with the original weights from Mousavi et al. (2020) can be loaded.\n",
        "\n",
        "    .. document_args:: seisbench.models EQTransformer\n",
        "\n",
        "    :param in_channels: Number of input channels, by default 3.\n",
        "    :param in_samples: Number of input samples per channel, by default 6000.\n",
        "                       The model expects input shape (in_channels, in_samples)\n",
        "    :param classes: Number of output classes, by default 2. The detection channel is not counted.\n",
        "    :param phases: Phase hints for the classes, by default \"PS\". Can be None.\n",
        "    :param res_cnn_blocks: Number of residual convolutional blocks\n",
        "    :param lstm_blocks: Number of LSTM blocks\n",
        "    :param drop_rate: Dropout rate\n",
        "    :param original_compatible: If True, uses a few custom layers for binary compatibility with original model\n",
        "                                from Mousavi et al. (2020).\n",
        "                                This option defaults to False.\n",
        "                                It is usually recommended to stick to the default value, as the custom layers show\n",
        "                                slightly worse performance than the PyTorch builtins.\n",
        "                                The exception is when loading the original weights using :py:func:`from_pretrained`.\n",
        "    :param norm: Data normalization strategy, either \"peak\" or \"std\".\n",
        "    :param kwargs: Keyword arguments passed to the constructor of :py:class:`WaveformModel`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        in_samples=6000,\n",
        "        classes=2,\n",
        "        phases=\"PS\",\n",
        "        lstm_blocks=3,\n",
        "        drop_rate=0.1,\n",
        "        original_compatible=False,\n",
        "        sampling_rate=100,\n",
        "        norm=\"std\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_samples = in_samples\n",
        "        self.in_channels = in_channels\n",
        "        self.classes = classes\n",
        "        self.lstm_blocks = lstm_blocks\n",
        "        self.drop_rate = drop_rate\n",
        "        self.norm = norm\n",
        "\n",
        "        # Add options for conservative and the true original - see https://github.com/seisbench/seisbench/issues/96#issuecomment-1155158224\n",
        "        if original_compatible == True:\n",
        "            warnings.warn(\n",
        "                \"Using the non-conservative 'original' model, set `original_compatible='conservative' to use the more conservative model\"\n",
        "            )\n",
        "            original_compatible = \"non-conservative\"\n",
        "\n",
        "        if original_compatible:\n",
        "            eps = 1e-7  # See Issue #96 - original models use tensorflow default epsilon of 1e-7\n",
        "        else:\n",
        "            eps = 1e-5\n",
        "        self.original_compatible = original_compatible\n",
        "\n",
        "        if original_compatible and in_samples != 6000:\n",
        "            raise ValueError(\"original_compatible=True requires in_samples=6000.\")\n",
        "\n",
        "        self._phases = phases\n",
        "        if phases is not None and len(phases) != classes:\n",
        "            raise ValueError(\n",
        "                f\"Number of classes ({classes}) does not match number of phases ({len(phases)}).\"\n",
        "            )\n",
        "\n",
        "        # Parameters from EQTransformer repository\n",
        "        self.filters = [\n",
        "            8,\n",
        "            16,\n",
        "            16,\n",
        "            32,\n",
        "            32,\n",
        "            64,\n",
        "            64,\n",
        "        ]  # Number of filters for the convolutions\n",
        "        self.kernel_sizes = [11, 9, 7, 7, 5, 5, 3]  # Kernel sizes for the convolutions\n",
        "        self.res_cnn_kernels = [3, 3, 3, 3, 2, 3, 2]\n",
        "\n",
        "        # TODO: Add regularizers when training model\n",
        "        # kernel_regularizer=keras.regularizers.l2(1e-6),\n",
        "        # bias_regularizer=keras.regularizers.l1(1e-4),\n",
        "\n",
        "        # Encoder stack\n",
        "        self.encoder = Encoder(\n",
        "            input_channels=self.in_channels,\n",
        "            filters=self.filters,\n",
        "            kernel_sizes=self.kernel_sizes,\n",
        "            in_samples=self.in_samples,\n",
        "        )\n",
        "\n",
        "        # Res CNN Stack\n",
        "        self.res_cnn_stack = ResCNNStack(\n",
        "            kernel_sizes=self.res_cnn_kernels,\n",
        "            filters=self.filters[-1],\n",
        "            drop_rate=self.drop_rate,\n",
        "        )\n",
        "\n",
        "        # BiLSTM stack\n",
        "        self.bi_lstm_stack = BiLSTMStack(\n",
        "            blocks=self.lstm_blocks,\n",
        "            input_size=self.filters[-1],\n",
        "            drop_rate=self.drop_rate,\n",
        "            original_compatible=original_compatible,\n",
        "        )\n",
        "\n",
        "        # Global attention - two transformers\n",
        "        self.transformer_d0 = Transformer(\n",
        "            input_size=16, drop_rate=self.drop_rate, eps=eps\n",
        "        )\n",
        "        self.transformer_d = Transformer(\n",
        "            input_size=16, drop_rate=self.drop_rate, eps=eps\n",
        "        )\n",
        "\n",
        "        # Detection decoder and final Conv\n",
        "        self.decoder_d = Decoder(\n",
        "            input_channels=16,\n",
        "            filters=self.filters[::-1],\n",
        "            kernel_sizes=self.kernel_sizes[::-1],\n",
        "            out_samples=in_samples,\n",
        "            original_compatible=original_compatible,\n",
        "        )\n",
        "        self.conv_d = nn.Conv1d(\n",
        "            in_channels=self.filters[0], out_channels=1, kernel_size=11, padding=5\n",
        "        )\n",
        "\n",
        "        # Picking branches\n",
        "        self.pick_lstms = []\n",
        "        self.pick_attentions = []\n",
        "        self.pick_decoders = []\n",
        "        self.pick_convs = []\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "        for _ in range(self.classes):\n",
        "            if original_compatible == \"conservative\":\n",
        "                # The non-conservative model uses a sigmoid activiation as handled by the base nn.LSTM\n",
        "                lstm = CustomLSTM(ActivationLSTMCell, 16, 16, bidirectional=False)\n",
        "            else:\n",
        "                lstm = nn.LSTM(16, 16, bidirectional=False)\n",
        "            self.pick_lstms.append(lstm)\n",
        "\n",
        "            attention = SeqSelfAttention(input_size=16, attention_width=3, eps=eps)\n",
        "            self.pick_attentions.append(attention)\n",
        "\n",
        "            decoder = Decoder(\n",
        "                input_channels=16,\n",
        "                filters=self.filters[::-1],\n",
        "                kernel_sizes=self.kernel_sizes[::-1],\n",
        "                out_samples=in_samples,\n",
        "                original_compatible=original_compatible,\n",
        "            )\n",
        "            self.pick_decoders.append(decoder)\n",
        "\n",
        "            conv = nn.Conv1d(\n",
        "                in_channels=self.filters[0], out_channels=1, kernel_size=11, padding=5\n",
        "            )\n",
        "            self.pick_convs.append(conv)\n",
        "\n",
        "        self.pick_lstms = nn.ModuleList(self.pick_lstms)\n",
        "        self.pick_attentions = nn.ModuleList(self.pick_attentions)\n",
        "        self.pick_decoders = nn.ModuleList(self.pick_decoders)\n",
        "        self.pick_convs = nn.ModuleList(self.pick_convs)\n",
        "\n",
        "    def forward(self, x, logits=False):\n",
        "        assert x.ndim == 3\n",
        "        assert x.shape[1:] == (self.in_channels, self.in_samples)\n",
        "\n",
        "        # Shared encoder part\n",
        "        x = self.encoder(x)\n",
        "        x = self.res_cnn_stack(x)\n",
        "        x = self.bi_lstm_stack(x)\n",
        "        x, _ = self.transformer_d0(x)\n",
        "        x, _ = self.transformer_d(x)\n",
        "\n",
        "        # Detection part\n",
        "        detection = self.decoder_d(x)\n",
        "        if logits:\n",
        "            detection = self.conv_d(detection)\n",
        "        else:\n",
        "            detection = torch.sigmoid(self.conv_d(detection))\n",
        "        detection = torch.squeeze(detection, dim=1)  # Remove channel dimension\n",
        "\n",
        "        outputs = [detection]\n",
        "\n",
        "        # Pick parts\n",
        "        for lstm, attention, decoder, conv in zip(\n",
        "            self.pick_lstms, self.pick_attentions, self.pick_decoders, self.pick_convs\n",
        "        ):\n",
        "            px = x.permute(\n",
        "                2, 0, 1\n",
        "            )  # From batch, channels, sequence to sequence, batch, channels\n",
        "            px = lstm(px)[0]\n",
        "            px = self.dropout(px)\n",
        "            px = px.permute(\n",
        "                1, 2, 0\n",
        "            )  # From sequence, batch, channels to batch, channels, sequence\n",
        "            px, _ = attention(px)\n",
        "            px = decoder(px)\n",
        "            if logits:\n",
        "                pred = conv(px)\n",
        "            else:\n",
        "                pred = torch.sigmoid(conv(px))\n",
        "            pred = torch.squeeze(pred, dim=1)  # Remove channel dimension\n",
        "\n",
        "            outputs.append(pred)\n",
        "\n",
        "        return tuple(outputs)\n",
        "\n",
        "        # Cosine taper (very short, i.e., only six samples on each side)\n",
        "        tap = 0.5 * (1 + np.cos(np.linspace(np.pi, 2 * np.pi, 6)))\n",
        "        window[:, :6] *= tap\n",
        "        window[:, -6:] *= tap[::-1]\n",
        "\n",
        "        return window\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder stack\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_channels, filters, kernel_sizes, in_samples):\n",
        "        super().__init__()\n",
        "\n",
        "        convs = []\n",
        "        pools = []\n",
        "        self.paddings = []\n",
        "        for in_channels, out_channels, kernel_size in zip(\n",
        "            [input_channels] + filters[:-1], filters, kernel_sizes\n",
        "        ):\n",
        "            convs.append(\n",
        "                nn.Conv1d(\n",
        "                    in_channels, out_channels, kernel_size, padding=kernel_size // 2\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # To be consistent with the behaviour in tensorflow,\n",
        "            # padding needs to be added for odd numbers of input_samples\n",
        "            padding = in_samples % 2\n",
        "\n",
        "            # Padding for MaxPool1d needs to be handled manually to conform with tf padding\n",
        "            self.paddings.append(padding)\n",
        "            pools.append(nn.MaxPool1d(2, padding=0))\n",
        "            in_samples = (in_samples + padding) // 2\n",
        "\n",
        "        self.convs = nn.ModuleList(convs)\n",
        "        self.pools = nn.ModuleList(pools)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for conv, pool, padding in zip(self.convs, self.pools, self.paddings):\n",
        "            x = torch.relu(conv(x))\n",
        "            if padding != 0:\n",
        "                # Only pad right, use -1e10 as negative infinity\n",
        "                x = F.pad(x, (0, padding), \"constant\", -1e10)\n",
        "            x = pool(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels,\n",
        "        filters,\n",
        "        kernel_sizes,\n",
        "        out_samples,\n",
        "        original_compatible=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
        "        self.original_compatible = original_compatible\n",
        "\n",
        "        # We need to trim off the final sample sometimes to get to the right number of output samples\n",
        "        self.crops = []\n",
        "        current_samples = out_samples\n",
        "        for i, _ in enumerate(filters):\n",
        "            padding = current_samples % 2\n",
        "            current_samples = (current_samples + padding) // 2\n",
        "            if padding == 1:\n",
        "                self.crops.append(len(filters) - 1 - i)\n",
        "\n",
        "        convs = []\n",
        "        for in_channels, out_channels, kernel_size in zip(\n",
        "            [input_channels] + filters[:-1], filters, kernel_sizes\n",
        "        ):\n",
        "            convs.append(\n",
        "                nn.Conv1d(\n",
        "                    in_channels, out_channels, kernel_size, padding=kernel_size // 2\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.convs = nn.ModuleList(convs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            x = self.upsample(x)\n",
        "\n",
        "            if self.original_compatible:\n",
        "                if i == 3:\n",
        "                    x = x[:, :, 1:-1]\n",
        "            else:\n",
        "                if i in self.crops:\n",
        "                    x = x[:, :, :-1]\n",
        "\n",
        "            x = F.relu(conv(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResCNNStack(nn.Module):\n",
        "    def __init__(self, kernel_sizes, filters, drop_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        members = []\n",
        "        for ker in kernel_sizes:\n",
        "            members.append(ResCNNBlock(filters, ker, drop_rate))\n",
        "\n",
        "        self.members = nn.ModuleList(members)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for member in self.members:\n",
        "            x = member(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResCNNBlock(nn.Module):\n",
        "    def __init__(self, filters, ker, drop_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        self.manual_padding = False\n",
        "        if ker == 3:\n",
        "            padding = 1\n",
        "        else:\n",
        "            # ker == 2\n",
        "            # Manual padding emulate the padding in tensorflow\n",
        "            self.manual_padding = True\n",
        "            padding = 0\n",
        "\n",
        "        self.dropout = SpatialDropout1d(drop_rate)\n",
        "\n",
        "        self.norm1 = nn.BatchNorm1d(filters, eps=1e-3)\n",
        "        self.conv1 = nn.Conv1d(filters, filters, ker, padding=padding)\n",
        "\n",
        "        self.norm2 = nn.BatchNorm1d(filters, eps=1e-3)\n",
        "        self.conv2 = nn.Conv1d(filters, filters, ker, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.norm1(x)\n",
        "        y = F.relu(y)\n",
        "        y = self.dropout(y)\n",
        "        if self.manual_padding:\n",
        "            y = F.pad(y, (0, 1), \"constant\", 0)\n",
        "        y = self.conv1(y)\n",
        "\n",
        "        y = self.norm2(y)\n",
        "        y = F.relu(y)\n",
        "        y = self.dropout(y)\n",
        "        if self.manual_padding:\n",
        "            y = F.pad(y, (0, 1), \"constant\", 0)\n",
        "        y = self.conv2(y)\n",
        "\n",
        "        return x + y\n",
        "\n",
        "\n",
        "class BiLSTMStack(nn.Module):\n",
        "    def __init__(\n",
        "        self, blocks, input_size, drop_rate, hidden_size=16, original_compatible=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # First LSTM has a different input size as the subsequent ones\n",
        "        self.members = nn.ModuleList(\n",
        "            [\n",
        "                BiLSTMBlock(\n",
        "                    input_size,\n",
        "                    hidden_size,\n",
        "                    drop_rate,\n",
        "                    original_compatible=original_compatible,\n",
        "                )\n",
        "            ]\n",
        "            + [\n",
        "                BiLSTMBlock(\n",
        "                    hidden_size,\n",
        "                    hidden_size,\n",
        "                    drop_rate,\n",
        "                    original_compatible=original_compatible,\n",
        "                )\n",
        "                for _ in range(blocks - 1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for member in self.members:\n",
        "            x = member(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BiLSTMBlock(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, drop_rate, original_compatible=False):\n",
        "        super().__init__()\n",
        "\n",
        "        if original_compatible == \"conservative\":\n",
        "            # The non-conservative model uses a sigmoid activiation as handled by the base nn.LSTM\n",
        "            self.lstm = CustomLSTM(ActivationLSTMCell, input_size, hidden_size)\n",
        "        elif original_compatible == \"non-conservative\":\n",
        "            self.lstm = CustomLSTM(\n",
        "                ActivationLSTMCell,\n",
        "                input_size,\n",
        "                hidden_size,\n",
        "                gate_activation=torch.sigmoid,\n",
        "            )\n",
        "        else:\n",
        "            self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "        self.conv = nn.Conv1d(2 * hidden_size, hidden_size, 1)\n",
        "        self.norm = nn.BatchNorm1d(hidden_size, eps=1e-3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(\n",
        "            2, 0, 1\n",
        "        )  # From batch, channels, sequence to sequence, batch, channels\n",
        "        x = self.lstm(x)[0]\n",
        "        x = self.dropout(x)\n",
        "        x = x.permute(\n",
        "            1, 2, 0\n",
        "        )  # From sequence, batch, channels to batch, channels, sequence\n",
        "        x = self.conv(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_size, drop_rate, attention_width=None, eps=1e-5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = SeqSelfAttention(\n",
        "            input_size, attention_width=attention_width, eps=eps\n",
        "        )\n",
        "        self.norm1 = LayerNormalization(input_size)\n",
        "        self.ff = FeedForward(input_size, drop_rate)\n",
        "        self.norm2 = LayerNormalization(input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y, weight = self.attention(x)\n",
        "        y = x + y\n",
        "        y = self.norm1(y)\n",
        "        y2 = self.ff(y)\n",
        "        y2 = y + y2\n",
        "        y2 = self.norm2(y2)\n",
        "\n",
        "        return y2, weight\n",
        "\n",
        "\n",
        "class SeqSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Additive self attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, units=32, attention_width=None, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.attention_width = attention_width\n",
        "\n",
        "        self.Wx = nn.Parameter(uniform(-0.02, 0.02, input_size, units))\n",
        "        self.Wt = nn.Parameter(uniform(-0.02, 0.02, input_size, units))\n",
        "        self.bh = nn.Parameter(torch.zeros(units))\n",
        "\n",
        "        self.Wa = nn.Parameter(uniform(-0.02, 0.02, units, 1))\n",
        "        self.ba = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape == (batch, channels, time)\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # to (batch, time, channels)\n",
        "\n",
        "        q = torch.unsqueeze(\n",
        "            torch.matmul(x, self.Wt), 2\n",
        "        )  # Shape (batch, time, 1, channels)\n",
        "        k = torch.unsqueeze(\n",
        "            torch.matmul(x, self.Wx), 1\n",
        "        )  # Shape (batch, 1, time, channels)\n",
        "\n",
        "        h = torch.tanh(q + k + self.bh)\n",
        "\n",
        "        # Emissions\n",
        "        e = torch.squeeze(\n",
        "            torch.matmul(h, self.Wa) + self.ba, -1\n",
        "        )  # Shape (batch, time, time)\n",
        "\n",
        "        # This is essentially softmax with an additional attention component.\n",
        "        e = (\n",
        "            e - torch.max(e, dim=-1, keepdim=True).values\n",
        "        )  # In versions <= 0.2.1 e was incorrectly normalized by max(x)\n",
        "        e = torch.exp(e)\n",
        "        if self.attention_width is not None:\n",
        "            lower = (\n",
        "                torch.arange(0, e.shape[1], device=e.device) - self.attention_width // 2\n",
        "            )\n",
        "            upper = lower + self.attention_width\n",
        "            indices = torch.unsqueeze(torch.arange(0, e.shape[1], device=e.device), 1)\n",
        "            mask = torch.logical_and(lower <= indices, indices < upper)\n",
        "            e = torch.where(mask, e, torch.zeros_like(e))\n",
        "\n",
        "        a = e / (torch.sum(e, dim=-1, keepdim=True) + self.eps)\n",
        "\n",
        "        v = torch.matmul(a, x)\n",
        "\n",
        "        v = v.permute(0, 2, 1)  # to (batch, channels, time)\n",
        "\n",
        "        return v, a\n",
        "\n",
        "\n",
        "def uniform(a, b, *args):\n",
        "    return a + (b - a) * torch.rand(*args)\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, filters, eps=1e-14):\n",
        "        super().__init__()\n",
        "\n",
        "        gamma = torch.ones(filters, 1)\n",
        "        self.gamma = nn.Parameter(gamma)\n",
        "        beta = torch.zeros(filters, 1)\n",
        "        self.beta = nn.Parameter(beta)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = torch.mean(x, 1, keepdim=True)\n",
        "        var = torch.mean((x - mean) ** 2, 1, keepdim=True) + self.eps\n",
        "        std = torch.sqrt(var)\n",
        "        outputs = (x - mean) / std\n",
        "\n",
        "        outputs = outputs * self.gamma\n",
        "        outputs = outputs + self.beta\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, io_size, drop_rate, hidden_size=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin1 = nn.Linear(io_size, hidden_size)\n",
        "        self.lin2 = nn.Linear(hidden_size, io_size)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # To (batch, time, channel)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.lin2(x)\n",
        "        x = x.permute(0, 2, 1)  # To (batch, channel, time)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialDropout1d(nn.Module):\n",
        "    def __init__(self, drop_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        self.drop_rate = drop_rate\n",
        "        self.dropout = nn.Dropout2d(drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(dim=-1)  # Add fake dimension\n",
        "        x = self.dropout(x)\n",
        "        x = x.squeeze(dim=-1)  # Remove fake dimension\n",
        "        return x"
      ],
      "metadata": {
        "id": "WDQ6V7ZXTyiQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EQTransformer(in_channels=1, in_samples=300)"
      ],
      "metadata": {
        "id": "PGuu0bwZWULj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pulse_width_range = [10,30]\n",
        "pulse_height_range = [3,10]\n",
        "pulse_start_range = [50,250]\n",
        "gauss_width = 11\n",
        "onset_shift = 0 # int(gauss_width/2)\n",
        "\n",
        "input,target = [],[]\n",
        "\n",
        "for i in range(1000):\n",
        "  # e = signal.gaussian(random.randint(pulse_width_range[0],pulse_width_range[1]), std=1)\n",
        "  e = signal.gaussian(gauss_width, std=2)\n",
        "\n",
        "  noise = np.random.normal(0, 1, 300)\n",
        "\n",
        "  start = random.randint(pulse_start_range[0],pulse_start_range[1])\n",
        "  noisy_pulse = np.copy(noise)\n",
        "  noisy_pulse[start+onset_shift:start+e.shape[0]+onset_shift] += random.randint(pulse_height_range[0],pulse_height_range[1])*e\n",
        "  # noisy_pulse *= [-1,1][random.randrange(2)]\n",
        "  label= np.zeros_like(noisy_pulse)\n",
        "  label[start:start+e.shape[0]] += e\n",
        "\n",
        "  input.append(noisy_pulse)\n",
        "  target.append(label)\n",
        "\n",
        "input = np.expand_dims(input, axis=-1)\n",
        "input = np.expand_dims(input, axis=-1)\n",
        "\n",
        "target = np.expand_dims(target, axis=-1)\n",
        "target = np.expand_dims(target, axis=-1)\n",
        "target = np.concatenate((target,1 - target),axis=3)\n",
        "\n",
        "print('Shape input:',np.shape(input))\n",
        "\n",
        "input_test,target_test = [],[]\n",
        "\n",
        "for i in range(128):\n",
        "  e = signal.gaussian(gauss_width, std=2)\n",
        "\n",
        "  noise = np.random.normal(0, 1, 300)\n",
        "\n",
        "  start = random.randint(pulse_start_range[0],pulse_start_range[1])\n",
        "  noisy_pulse = np.copy(noise)\n",
        "  noisy_pulse[start+onset_shift:start+e.shape[0]+onset_shift] += random.randint(pulse_height_range[0],pulse_height_range[1])*e\n",
        "  # noisy_pulse *= [-1,1][random.randrange(2)]\n",
        "\n",
        "  label= np.zeros_like(noisy_pulse)\n",
        "  label[start:start+e.shape[0]] += e\n",
        "\n",
        "  input_test.append(noisy_pulse)\n",
        "  target_test.append(label)\n",
        "\n",
        "input_test = np.expand_dims(input_test, axis=-1)\n",
        "input_test = np.expand_dims(input_test, axis=-1)\n",
        "\n",
        "target_test = np.expand_dims(target_test, axis=-1)\n",
        "target_test = np.expand_dims(target_test, axis=-1)\n",
        "target_test = np.concatenate((target_test,1 - target_test),axis=3)\n",
        "\n",
        "print(input.shape, target.shape, input_test.shape, target_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3iOjiUZYnee",
        "outputId": "b960c067-c6b9-4fd8-97d0-f7764b3de978"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape input: (1000, 300, 1, 1)\n",
            "(1000, 300, 1, 1) (1000, 300, 1, 2) (128, 300, 1, 1) (128, 300, 1, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-604582f2ba82>:11: DeprecationWarning: Importing gaussian from 'scipy.signal' is deprecated and will raise an error in SciPy 1.13.0. Please use 'scipy.signal.windows.gaussian' or the convenience function 'scipy.signal.get_window' instead.\n",
            "  e = signal.gaussian(gauss_width, std=2)\n",
            "<ipython-input-45-604582f2ba82>:37: DeprecationWarning: Importing gaussian from 'scipy.signal' is deprecated and will raise an error in SciPy 1.13.0. Please use 'scipy.signal.windows.gaussian' or the convenience function 'scipy.signal.get_window' instead.\n",
            "  e = signal.gaussian(gauss_width, std=2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "input_tensor = torch.tensor(input.transpose(0, 3, 1, 2), dtype=torch.float32).squeeze(3)\n",
        "target_tensor = torch.tensor(target.transpose(0, 3, 1, 2), dtype=torch.float32).squeeze(3)\n",
        "input_test_tensor = torch.tensor(input_test.transpose(0, 3, 1, 2), dtype=torch.float32).squeeze(3)\n",
        "target_test_tensor = torch.tensor(target_test.transpose(0, 3, 1, 2), dtype=torch.float32).squeeze(3)"
      ],
      "metadata": {
        "id": "d4qkBDZTY814"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(input_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv_hi0RuZVym",
        "outputId": "612054e9-9fd2-4173-ffdb-9fd6d31baf93"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.5133, 0.5110, 0.5100,  ..., 0.5120, 0.5147, 0.5176],\n",
              "         [0.5133, 0.5110, 0.5100,  ..., 0.5119, 0.5147, 0.5176],\n",
              "         [0.5133, 0.5110, 0.5101,  ..., 0.5120, 0.5147, 0.5176],\n",
              "         ...,\n",
              "         [0.5133, 0.5110, 0.5100,  ..., 0.5120, 0.5147, 0.5177],\n",
              "         [0.5133, 0.5110, 0.5100,  ..., 0.5120, 0.5147, 0.5176],\n",
              "         [0.5133, 0.5110, 0.5100,  ..., 0.5120, 0.5147, 0.5176]],\n",
              "        grad_fn=<SqueezeBackward1>),\n",
              " tensor([[0.5167, 0.5139, 0.5128,  ..., 0.5126, 0.5139, 0.5150],\n",
              "         [0.5167, 0.5139, 0.5128,  ..., 0.5126, 0.5139, 0.5150],\n",
              "         [0.5167, 0.5139, 0.5128,  ..., 0.5126, 0.5139, 0.5150],\n",
              "         ...,\n",
              "         [0.5167, 0.5139, 0.5128,  ..., 0.5126, 0.5139, 0.5150],\n",
              "         [0.5167, 0.5139, 0.5128,  ..., 0.5126, 0.5139, 0.5150],\n",
              "         [0.5167, 0.5139, 0.5128,  ..., 0.5126, 0.5139, 0.5150]],\n",
              "        grad_fn=<SqueezeBackward1>),\n",
              " tensor([[0.5079, 0.5084, 0.5088,  ..., 0.5119, 0.5118, 0.5112],\n",
              "         [0.5079, 0.5084, 0.5088,  ..., 0.5119, 0.5118, 0.5113],\n",
              "         [0.5079, 0.5084, 0.5088,  ..., 0.5119, 0.5118, 0.5113],\n",
              "         ...,\n",
              "         [0.5079, 0.5084, 0.5088,  ..., 0.5119, 0.5118, 0.5112],\n",
              "         [0.5079, 0.5084, 0.5088,  ..., 0.5119, 0.5118, 0.5113],\n",
              "         [0.5079, 0.5084, 0.5088,  ..., 0.5119, 0.5118, 0.5113]],\n",
              "        grad_fn=<SqueezeBackward1>))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yHDPlZaNZXRj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}